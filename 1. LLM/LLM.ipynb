{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQFArNKe8d2c",
        "outputId": "c291e991-f69a-4a7b-ead0-cb567e2369b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import json\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "Q_NhLAEg9bHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"\"\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "print(\"Client initialized successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWM8MuVD9c5w",
        "outputId": "ad0d6ed4-2c1c-4a0d-d201-41f10c5347e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, temperature=0.7):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "-uqcG-ui8nwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_json(prompt, response_text):\n",
        "    data = {\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response_text,\n",
        "        \"timestamp\": str(datetime.now())\n",
        "    }\n",
        "\n",
        "    file_name = \"chat_history.json\"\n",
        "\n",
        "    try:\n",
        "        with open(file_name, \"r\") as f:\n",
        "            existing_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        existing_data = []\n",
        "\n",
        "    existing_data.append(data)\n",
        "\n",
        "    with open(file_name, \"w\") as f:\n",
        "        json.dump(existing_data, f, indent=4)\n",
        "\n",
        "    print(\"Saved to chat_history.json\")\n"
      ],
      "metadata": {
        "id": "HDBhRblM9FOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accept user input\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "print(\"\\nGenerating response...\\n\")\n",
        "\n",
        "# Generate response\n",
        "response = generate_text(prompt)\n",
        "\n",
        "# Display response\n",
        "print(\"AI Response:\\n\")\n",
        "print(response)\n",
        "\n",
        "# Save to JSON\n",
        "save_to_json(prompt, response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTWo-dk9IQV",
        "outputId": "130ed970-0c2b-4b88-b030-b92d36c93dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: what is llm\n",
            "\n",
            "Generating response...\n",
            "\n",
            "AI Response:\n",
            "\n",
            "**LLM** stands for **Large Language Model**.\n",
            "\n",
            "| What it is | How it works | Why it matters |\n",
            "|------------|--------------|----------------|\n",
            "| A type of artificial‑intelligence model that learns to predict and generate human‑like text | • Trained on massive amounts of text data (books, articles, web pages, code, etc.)<br>• Uses transformer architecture (attention mechanisms) to understand context and relationships between words<br>• Learns statistical patterns and semantic meaning without explicit programming | • Enables natural‑language understanding and generation (chatbots, translation, summarization, code completion, etc.)<br>• Drives many modern AI applications: virtual assistants, content creation tools, AI‑powered coding helpers, etc.<br>• Continues to improve as models grow larger and training data diversifies |\n",
            "\n",
            "### Core concepts\n",
            "\n",
            "1. **Transformer architecture**  \n",
            "   - Introduced by Vaswani et al. in 2017.  \n",
            "   - Uses self‑attention to capture relationships across all words in a sentence, regardless of distance.\n",
            "\n",
            "2. **Pre‑training + Fine‑tuning**  \n",
            "   - **Pre‑training**: The model is first trained on a general language task (e.g., predicting the next word).  \n",
            "   - **Fine‑tuning**: It is then specialized on a narrower task (e.g., answering medical questions) by continuing training on domain‑specific data.\n",
            "\n",
            "3. **Scale matters**  \n",
            "   - The “large” in LLM refers to the number of parameters (weights) and the amount of training data.  \n",
            "   - Larger models tend to capture more nuanced language patterns and perform better on a wide range of tasks.\n",
            "\n",
            "### Typical uses\n",
            "\n",
            "| Domain | Example Applications |\n",
            "|--------|----------------------|\n",
            "| **Customer support** | AI chatbots that answer FAQs. |\n",
            "| **Content creation** | Drafting articles, generating marketing copy, composing poetry. |\n",
            "| **Programming** | Code generation, bug detection, documentation. |\n",
            "| **Education** | Tutoring systems, automated grading. |\n",
            "| **Research** | Summarizing literature, generating hypotheses. |\n",
            "\n",
            "### Popular LLMs (as of 2026)\n",
            "\n",
            "| Model | Size (parameters) | Notable features |\n",
            "|-------|-------------------|------------------|\n",
            "| GPT‑4 (OpenAI) | ~175 B | Multimodal (text + image) in some variants. |\n",
            "| Claude 3 (Anthropic) | ~1‑2 T | Focus on safety and interpretability. |\n",
            "| Gemini Pro (Google) | ~2 T | Integrated with Google Workspace tools. |\n",
            "| LLaMA‑3 (Meta) | 70 B – 1 T | Open‑source, community‑friendly. |\n",
            "| PaLM‑2 (Google) | 540 B | Strong in reasoning and code. |\n",
            "\n",
            "### Ethical and practical considerations\n",
            "\n",
            "- **Bias & fairness** – Models can inherit biases present in training data.  \n",
            "- **Misuse** – Potential for generating disinformation, spam, or harmful content.  \n",
            "- **Energy consumption** – Training huge models consumes significant computational resources.  \n",
            "- **Interpretability** – It can be hard to know why a model makes a particular decision.  \n",
            "\n",
            "### Bottom line\n",
            "\n",
            "An LLM is a powerful, data‑driven AI system that can understand and produce natural language. Its ability to learn from vast amounts of text makes it useful across many fields, but it also raises important technical, ethical, and environmental questions that researchers and practitioners are actively addressing.\n",
            "Saved to chat_history.json\n"
          ]
        }
      ]
    }
  ]
}